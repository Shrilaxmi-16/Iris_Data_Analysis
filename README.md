
# Iris Dataset Classification using KNN  

This project demonstrates **data analysis, preprocessing, and classification** on the famous **Iris dataset**. We use **K-Nearest Neighbors (KNN)** as the classifier, experiment with different values of *K*, evaluate the model, and visualize the decision boundaries.  

## Dataset  
**Source:** UCI Machine Learning Repository ‚Äì Iris Dataset  

- **Species (Target Variable):**  
  - Setosa  
  - Versicolor  
  - Virginica  

- **Features:**  
  - Sepal Length (cm)  
  - Sepal Width (cm)  
  - Petal Length (cm)  
  - Petal Width (cm)  

- **Id:** Identifier column (dropped as irrelevant).  


## Exploratory Data Analysis  

### Data Overview  
- Loaded the dataset and reviewed its structure and statistics.  
- Dropped the `Id` column as it does not contribute to prediction.  
- Confirmed there were **no missing values** or inconsistencies in the data.  

### Feature Distribution Analysis  
- Plotted histograms and density plots for numerical features.  
- **Setosa** species showed clear separation in petal dimensions, while **Versicolor** and **Virginica** overlapped.  
- Identified potential skewness in certain features.  

### Correlation Analysis  
- Generated correlation matrix for all numeric features.  
- Heatmap highlighted:  
  - Strong correlation between **Petal Length and Petal Width**.  
  - Moderate correlations between Sepal Length and petal features.  
- Sepal Width showed weaker correlations with other variables.  

### Categorical Feature Analysis  
- Created count plots for the target variable (**Species**).  
- Dataset contained **50 samples per species**, ensuring balanced class distribution.  

### Boxplots for Feature Comparison  
- Compared numerical features across the three species:  
  - **Setosa:** Significantly smaller petals, distinct from the other two classes.  
  - **Versicolor:** Intermediate values, overlapping with Virginica.  
  - **Virginica:** Largest petal dimensions overall.  

### Feature Importance Visualization  
- Encoded target labels and trained a **Random Forest Classifier**.  
- Ranked features by importance:  
  - **Petal Length** and **Petal Width** were most significant.  
  - **Sepal features** contributed less to classification.  
- Visualized feature importances with a bar chart.  


## Model Analysis: KNN Classification  

### Model Training  
- Standardized features using **StandardScaler**.  
- Implemented **KNeighborsClassifier** with multiple values of `K` (1, 3, 5, 7, 9, 11).  

### Model Evaluation  
- Computed **accuracy scores** for each value of `K`.  
- Evaluated with **confusion matrix** and **classification report**.  
- Plotted **accuracy vs K curve**, identifying **K=5** as a stable choice.  

### Decision Boundary Visualization  
- **Raw Feature Visualization:**  
  - Used only Sepal Length and Sepal Width.  
  - Decision boundaries showed overlap between Versicolor and Virginica.  

- **PCA Visualization (All Features):**  
  - Reduced 4D data to 2D using PCA.  
  - Plotted decision boundaries with **K=5**.  
  - Displayed much clearer separation between all three species.  

## üìä Results  

- Accuracy varied slightly with different `K` values, with **K=5** giving stable results.  
- PCA-based visualization captured better class separations compared to raw features.  
- Decision boundaries clearly show the separation between **Setosa, Versicolor, and Virginica**.  


## üõ†Ô∏è Technologies Used  
- Python  
- Pandas, NumPy  
- Scikit-learn (KNeighborsClassifier, StandardScaler, PCA)  
- Matplotlib, Seaborn  


## üöÄ How to Run  

1. Clone the repository:  
   ```bash
   git clone https://github.com/your-username/iris-knn-classification.git
   cd iris-knn-classification


## Conclusion  
This analysis demonstrates the full workflow:  
**EDA ‚Üí Preprocessing ‚Üí Classification with KNN ‚Üí Evaluation ‚Üí Visualization**,  
showing how dimensionality reduction (PCA) improves visualization of complex class boundaries.  
